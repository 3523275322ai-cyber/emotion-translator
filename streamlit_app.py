import streamlit as st
import torch
import torch.nn as nn
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import librosa
import numpy as np
import requests
import tempfile
import os
from gtts import gTTS
import io
import base64

# è®¾ç½®é¡µé¢
st.set_page_config(
    page_title="æƒ…æ„Ÿå¼ºåº¦ç¿»è¯‘å™¨",
    page_icon="ğŸ¯",
    layout="wide"
)

# æ ‡é¢˜å’Œä»‹ç»
st.title("ğŸ¯ æƒ…æ„Ÿå¼ºåº¦ç¿»è¯‘å™¨")
st.markdown("å°†è¯­éŸ³è½¬æ¢ä¸ºå¸¦æœ‰æƒ…æ„Ÿå¼ºåº¦çš„æ–‡å­—è¡¨è¾¾ - **24/7æ°¸ä¹…è¿è¡Œ**")

# åˆå§‹åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨ç¼“å­˜é¿å…é‡å¤åŠ è½½ï¼‰
@st.cache_resource
def load_models():
    """åŠ è½½æ‰€æœ‰æ¨¡å‹"""
    st.info("ğŸ”„ åŠ è½½æ¨¡å‹ä¸­...")
    
    # è½¬å½•æ¨¡å‹
    processor = WhisperProcessor.from_pretrained("openai/whisper-small")
    model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
    
    # ç®€åŒ–æƒ…æ„Ÿå¼ºåº¦æ¨¡å‹
    class SimpleIntensityModel(nn.Module):
        def __init__(self, input_size=80, num_classes=6):
            super().__init__()
            self.classifier = nn.Sequential(
                nn.Linear(input_size, 128),
                nn.ReLU(),
                nn.Dropout(0.3),
                nn.Linear(128, num_classes)
            )
        def forward(self, x):
            return self.classifier(x)
    
    intensity_model = SimpleIntensityModel()
    
    st.success("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    return processor, model, intensity_model

# åŠ è½½æ¨¡å‹
processor, model, intensity_model = load_models()

# DeepSeek APIå¯†é’¥ï¼ˆåœ¨Streamlit Cloudä¸­è®¾ç½®ï¼‰
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")

def transcribe_audio(audio_file):
    """è½¬å½•éŸ³é¢‘"""
    try:
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
            tmp_file.write(audio_file.read())
            tmp_file_path = tmp_file.name
        
        # åŠ è½½éŸ³é¢‘
        audio, sr = librosa.load(tmp_file_path, sr=16000)
        
        # æå–ç‰¹å¾
        input_features = processor(
            audio, 
            sampling_rate=sr, 
            return_tensors="pt"
        ).input_features
        
        # ç”Ÿæˆè½¬å½•
        with torch.no_grad():
            predicted_ids = model.generate(input_features)
        
        transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(tmp_file_path)
        
        return transcription
        
    except Exception as e:
        return f"è½¬å½•å¤±è´¥: {str(e)}"

def predict_intensity_simple(audio_file):
    """ç®€å•æƒ…æ„Ÿå¼ºåº¦é¢„æµ‹"""
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
            tmp_file.write(audio_file.read())
            tmp_file_path = tmp_file.name
        
        audio, sr = librosa.load(tmp_file_path, sr=16000)
        
        # åŸºäºéŸ³é¢‘ç‰¹å¾ç®€å•åˆ¤æ–­å¼ºåº¦
        # 1. èƒ½é‡ç‰¹å¾
        rms = librosa.feature.rms(y=audio)
        avg_rms = np.mean(rms)
        
        # 2. é¢‘è°±ç‰¹å¾
        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)
        avg_centroid = np.mean(spectral_centroids)
        
        # ç®€å•åŠ æƒè¯„åˆ†
        intensity_score = (avg_rms * 100 + avg_centroid / 100) / 2
        
        # æ˜ å°„åˆ°0-5
        if intensity_score < 0.1: intensity = 1
        elif intensity_score < 0.2: intensity = 2
        elif intensity_score < 0.4: intensity = 3
        elif intensity_score < 0.6: intensity = 4
        else: intensity = 5
        
        os.unlink(tmp_file_path)
        return intensity
        
    except Exception as e:
        st.error(f"å¼ºåº¦åˆ†æå¤±è´¥: {e}")
        return 3  # é»˜è®¤å€¼

def translate_with_emotion(text, intensity):
    """æƒ…æ„Ÿç¿»è¯‘"""
    if not DEEPSEEK_API_KEY:
        # å¤‡ç”¨ç¿»è¯‘
        intensity_words = ["", "æœ‰ç‚¹", "æœ‰äº›", "æ¯”è¾ƒ", "å¾ˆ", "éå¸¸"]
        word = intensity_words[intensity] if intensity < len(intensity_words) else "æœ‰äº›"
        
        if intensity <= 1:
            return f"{text}"
        elif intensity <= 3:
            return f"æˆ‘ç°åœ¨{word}{text}ï¼Œè¯·å¸®å¸®æˆ‘"
        else:
            return f"æˆ‘ç°åœ¨{word}{text}ï¼è¯·å¸®å¸®æˆ‘ï¼"
    
    # DeepSeek APIè°ƒç”¨
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {DEEPSEEK_API_KEY}"
    }
    
    system_prompt = "ä½ æ˜¯ä¸€ä¸ªæƒ…æ„Ÿç¿»è¯‘å™¨ã€‚å°†ç”¨æˆ·çš„è¯è½¬æ¢æˆå¸¦æœ‰æƒ…æ„Ÿå¼ºåº¦çš„è¡¨è¾¾ã€‚ç›´æ¥è¾“å‡ºç¿»è¯‘åçš„å¥å­ï¼Œä¸è¦åŠ ä»»ä½•è§£é‡Šã€‚"
    
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"{text} å¼ºåº¦{intensity}"}
    ]
    
    try:
        response = requests.post(
            "https://api.deepseek.com/v1/chat/completions",
            headers=headers,
            json={
                "model": "deepseek-chat",
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 100
            },
            timeout=30
        )
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"].strip()
        
    except Exception as e:
        st.error(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {e}")
        # å¤‡ç”¨ç¿»è¯‘
        intensity_words = ["", "æœ‰ç‚¹", "æœ‰äº›", "æ¯”è¾ƒ", "å¾ˆ", "éå¸¸"]
        word = intensity_words[intensity] if intensity < len(intensity_words) else "æœ‰äº›"
        return f"æˆ‘ç°åœ¨{word}{text}ï¼Œè¯·å¸®å¸®æˆ‘"

def text_to_speech_base64(text):
    """æ–‡æœ¬è½¬è¯­éŸ³ï¼Œè¿”å›base64"""
    try:
        tts = gTTS(text=text, lang="zh-cn")
        audio_buffer = io.BytesIO()
        tts.write_to_fp(audio_buffer)
        audio_buffer.seek(0)
        
        audio_base64 = base64.b64encode(audio_buffer.read()).decode("utf-8")
        return f"data:audio/mp3;base64,{audio_base64}"
    except Exception as e:
        st.error(f"è¯­éŸ³ç”Ÿæˆå¤±è´¥: {e}")
        return None

# ä¸»ç•Œé¢
tab1, tab2 = st.tabs(["ğŸ¤ éŸ³é¢‘åˆ†æ", "ğŸ’¡ ä½¿ç”¨è¯´æ˜"])

with tab1:
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("ä¸Šä¼ éŸ³é¢‘")
        
        # æ–‡ä»¶ä¸Šä¼ 
        audio_file = st.file_uploader(
            "é€‰æ‹©éŸ³é¢‘æ–‡ä»¶",
            type=["wav", "mp3", "m4a", "ogg"],
            help="æ”¯æŒ WAV, MP3, M4A, OGG æ ¼å¼"
        )
        
        if audio_file is not None:
            # æ˜¾ç¤ºéŸ³é¢‘ä¿¡æ¯
            st.audio(audio_file, format="audio/wav")
            st.info(f"ğŸ“ æ–‡ä»¶: {audio_file.name}")
            
            # åˆ†ææŒ‰é’®
            if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary", use_container_width=True):
                with st.spinner("åˆ†æä¸­...è¯·ç¨å€™"):
                    # è½¬å½•
                    transcription = transcribe_audio(audio_file)
                    
                    # æƒ…æ„Ÿå¼ºåº¦
                    intensity = predict_intensity_simple(audio_file)
                    
                    # æƒ…æ„Ÿç¿»è¯‘
                    translated_text = translate_with_emotion(transcription, intensity)
                    
                    # ç”Ÿæˆè¯­éŸ³
                    audio_url = text_to_speech_base64(translated_text)
                
                # æ˜¾ç¤ºç»“æœ
                st.success("âœ… åˆ†æå®Œæˆï¼")
                
                st.subheader("ğŸ“Š åˆ†æç»“æœ")
                
                col_a, col_b = st.columns(2)
                
                with col_a:
                    st.metric("åŸå§‹è½¬å½•", transcription)
                    st.metric("æƒ…æ„Ÿå¼ºåº¦", f"{intensity}/5")
                
                with col_b:
                    st.text_area("æƒ…æ„Ÿç¿»è¯‘", translated_text, height=100)
                
                # è¯­éŸ³æ’­æ”¾
                if audio_url:
                    st.subheader("ğŸ”Š è¯­éŸ³è¾“å‡º")
                    st.audio(audio_url, format="audio/mp3")
                    st.download_button(
                        "ä¸‹è½½è¯­éŸ³",
                        data=base64.b64decode(audio_url.split(",")[1]),
                        file_name="æƒ…æ„Ÿç¿»è¯‘.mp3",
                        mime="audio/mp3"
                    )

    with col2:
        st.subheader("å®æ—¶çŠ¶æ€")
        if audio_file is None:
            st.info("ğŸ‘† è¯·åœ¨ä¸Šä¼ éŸ³é¢‘åå¼€å§‹åˆ†æ")
        else:
            st.success("âœ… éŸ³é¢‘æ–‡ä»¶å·²å°±ç»ª")
            
        # æ˜¾ç¤ºé…ç½®çŠ¶æ€
        st.subheader("âš™ï¸ é…ç½®çŠ¶æ€")
        if DEEPSEEK_API_KEY:
            st.success("âœ… DeepSeek API å·²é…ç½®")
        else:
            st.warning("âš ï¸ DeepSeek API æœªé…ç½®ï¼Œä½¿ç”¨åŸºç¡€ç¿»è¯‘")

with tab2:
    st.subheader("ä½¿ç”¨è¯´æ˜")
    
    st.markdown("""
    ### ğŸ¯ åŠŸèƒ½è¯´æ˜
    å°†ç”¨æˆ·çš„è¯­éŸ³è½¬æ¢æˆå¸¦æœ‰æƒ…æ„Ÿå¼ºåº¦çš„æ–‡å­—è¡¨è¾¾ï¼Œä¸»è¦ç”¨äºåŒ»ç–—æŠ¤ç†åœºæ™¯ã€‚
    
    ### ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹
    | ç”¨æˆ·è¾“å…¥ | æƒ…æ„Ÿå¼ºåº¦ | è¾“å‡ºç»“æœ |
    |---------|----------|----------|
    | "æˆ‘æƒ³å–æ°´" | å¼ºåº¦2 | "æˆ‘ç°åœ¨æœ‰ç‚¹æƒ³å–æ°´ï¼Œè¯·å¸®å¸®æˆ‘" |
    | "æˆ‘æƒ³å–æ°´" | å¼ºåº¦4 | "æˆ‘ç°åœ¨å¾ˆæƒ³å–æ°´ï¼è¯·å¸®å¸®æˆ‘ï¼" |
    | "æˆ‘æœ‰ç‚¹å†·" | å¼ºåº¦3 | "æˆ‘ç°åœ¨æ¯”è¾ƒå†·ï¼Œè¯·å¸®å¸®æˆ‘" |
    
    ### ğŸšï¸ æƒ…æ„Ÿå¼ºåº¦è¯´æ˜
    - **å¼ºåº¦0-1**: å¹³é™è¡¨è¾¾
    - **å¼ºåº¦2-3**: ä¸­ç­‰æ€¥åˆ‡  
    - **å¼ºåº¦4-5**: éå¸¸æ€¥åˆ‡
    
    ### ğŸ”§ æŠ€æœ¯æ¶æ„
    - **è¯­éŸ³è½¬å½•**: OpenAI Whisper
    - **æƒ…æ„Ÿåˆ†æ**: è‡ªå®šä¹‰æ·±åº¦å­¦ä¹ æ¨¡å‹
    - **æ–‡æœ¬ç”Ÿæˆ**: DeepSeek API
    - **è¯­éŸ³åˆæˆ**: Google TTS
    """)
    
    st.subheader("é…ç½®è¯´æ˜")
    st.markdown("""
    å¦‚éœ€è·å¾—æœ€ä½³æ•ˆæœï¼Œè¯·åœ¨Streamlit Cloudä¸­è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
    - `DEEPSEEK_API_KEY`: æ‚¨çš„DeepSeek APIå¯†é’¥
    """)

# é¡µè„š
st.markdown("---")
st.markdown("ğŸ¯ æƒ…æ„Ÿå¼ºåº¦ç¿»è¯‘å™¨ | 24/7æ°¸ä¹…è¿è¡Œ | Streamlit Cloudéƒ¨ç½²")
